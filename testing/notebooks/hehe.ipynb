{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt  # we only need pyplot\n",
    "sns.set()  # set the default Seaborn style for graphics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def train_model(tokenized_train_dataset, tokenized_test_dataset, label2id, id2label, model_path='microsoft/deberta-v3-small'):\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    )\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Define metrics computation\n",
    "    def compute_metrics(eval_pred):\n",
    "        labels = eval_pred.label_ids\n",
    "        predictions = eval_pred.predictions.argmax(-1)\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        f1 = f1_score(labels, predictions, average='micro')\n",
    "        precision = precision_score(labels, predictions, average='micro')\n",
    "        recall = recall_score(labels, predictions, average='micro')\n",
    "        return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "    # Training arguments with reduced verbosity\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"model\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=3,\n",
    "        per_device_eval_batch_size=3,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_dir='logs',  # Reduced verbosity in logging\n",
    "        logging_strategy='steps',  # Reduced verbosity in logging\n",
    "    )\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # Trainer setup\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(optimizer, None)\n",
    "    )\n",
    "\n",
    "    # Train and save the model\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(train_dataset, test_dataset, department2id):\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    def process_dataset(dataset, department2id):\n",
    "        texts = dataset[\"Description\"]\n",
    "        departments = [\n",
    "            col for col in dataset.column_names if col != 'Description']\n",
    "        labels_list = []\n",
    "\n",
    "        for example in dataset:\n",
    "            labels = [0. for _ in range(len(departments))]\n",
    "            for department in departments:\n",
    "                if example[department] == 1:\n",
    "                    label_id = department2id[department]\n",
    "                    labels[label_id] = 1.\n",
    "            labels_list.append(labels)\n",
    "\n",
    "        encoded_texts = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        encoded_dict = {key: val for key, val in encoded_texts.items()}\n",
    "        encoded_dict['labels'] = labels_list\n",
    "\n",
    "        return Dataset.from_dict(encoded_dict)\n",
    "\n",
    "    tokenized_train_dataset = process_dataset(train_dataset, department2id)\n",
    "    tokenized_test_dataset = process_dataset(test_dataset, department2id)\n",
    "\n",
    "    return tokenized_train_dataset, tokenized_test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 08:14:11.431338: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-21 08:14:11.471226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-21 08:14:12.322378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import logging\n",
    "from typing import Dict, Tuple\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import max_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_data(df: pd.DataFrame, parameters: Dict):\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=parameters[\"test_size\"], random_state=parameters[\"random_state\"])\n",
    "    return train_df, test_df\n",
    "\n",
    "def split_subcategory_data(encoded_dir: str, parameters: Dict) -> Dict[str, pd.DataFrame]:\n",
    "    # Load parameters\n",
    "    test_size = parameters[\"test_size\"]\n",
    "    random_state = parameters[\"random_state\"]\n",
    "\n",
    "    # Create a dictionary to store the partitioned data\n",
    "    partitioned_data = {}\n",
    "\n",
    "    # Iterate through each department's data in the encoded_dir\n",
    "    for department in parameters[\"departments\"]:\n",
    "        input_filepath = os.path.join(encoded_dir, f\"{department}.csv\")\n",
    "        df = pd.read_csv(input_filepath)\n",
    "\n",
    "        # Split the data\n",
    "        train_df, test_df = train_test_split(\n",
    "            df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        # Store the train and test data in the partitioned_data dictionary\n",
    "        partitioned_data[f\"{department}_train\"] = train_df\n",
    "        partitioned_data[f\"{department}_test\"] = test_df\n",
    "\n",
    "    return partitioned_data\n",
    "\n",
    "\n",
    "def dataframe_to_dataset(train, test):\n",
    "    train_dataset = Dataset.from_pandas(train)\n",
    "    test_dataset = Dataset.from_pandas(test)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def department_label_encoding(train_dataset) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    output_dir = \"data/04_feature/department_label_encoded_dir\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    labels = [col for col in train_dataset.column_names if col != 'Description']\n",
    "\n",
    "    label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "    return label2id, id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Biometric</th>\n",
       "      <th>Document</th>\n",
       "      <th>Asset Management</th>\n",
       "      <th>Data Security Incident</th>\n",
       "      <th>Off Boarding</th>\n",
       "      <th>CCTV</th>\n",
       "      <th>Network</th>\n",
       "      <th>Network and Infrastructure Design</th>\n",
       "      <th>Circuit</th>\n",
       "      <th>...</th>\n",
       "      <th>New Hire Shadowing</th>\n",
       "      <th>Web/Application Security</th>\n",
       "      <th>On Boarding</th>\n",
       "      <th>BCP Activation</th>\n",
       "      <th>Ticketing Support</th>\n",
       "      <th>Email Security</th>\n",
       "      <th>TPMO</th>\n",
       "      <th>Food Allowance</th>\n",
       "      <th>Procurement</th>\n",
       "      <th>Compliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>request reactiv user s biometr access</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>request upload access anoth biometr devic</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>request re assign badg card differ user</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>request badg card it alreadi present need</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>request disabl biometr access</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Description  Biometric  Document  \\\n",
       "0      request reactiv user s biometr access          1         0   \n",
       "1  request upload access anoth biometr devic          1         0   \n",
       "2    request re assign badg card differ user          1         0   \n",
       "3  request badg card it alreadi present need          1         0   \n",
       "4              request disabl biometr access          1         0   \n",
       "\n",
       "   Asset Management  Data Security Incident  Off Boarding  CCTV  Network  \\\n",
       "0                 0                       0             0     0        0   \n",
       "1                 0                       0             0     0        0   \n",
       "2                 0                       0             0     0        0   \n",
       "3                 0                       0             0     0        0   \n",
       "4                 0                       0             0     0        0   \n",
       "\n",
       "   Network and Infrastructure Design  Circuit  ...  New Hire Shadowing  \\\n",
       "0                                  0        0  ...                   0   \n",
       "1                                  0        0  ...                   0   \n",
       "2                                  0        0  ...                   0   \n",
       "3                                  0        0  ...                   0   \n",
       "4                                  0        0  ...                   0   \n",
       "\n",
       "   Web/Application Security  On Boarding  BCP Activation  Ticketing Support  \\\n",
       "0                         0            0               0                  0   \n",
       "1                         0            0               0                  0   \n",
       "2                         0            0               0                  0   \n",
       "3                         0            0               0                  0   \n",
       "4                         0            0               0                  0   \n",
       "\n",
       "   Email Security  TPMO  Food Allowance  Procurement  Compliance  \n",
       "0               0     0               0            0           0  \n",
       "1               0     0               0            0           0  \n",
       "2               0     0               0            0           0  \n",
       "3               0     0               0            0           0  \n",
       "4               0     0               0            0           0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '../data/02_intermediate/category_encoded_dir/Technology Services.csv'\n",
    "df = pd.read_csv(file, encoding='ISO-8859-1', skipinitialspace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataframe_to_dataset(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id, id2label = department_label_encoding(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Biometric': 0,\n",
       "  'Document': 1,\n",
       "  'Asset Management': 2,\n",
       "  'Data Security Incident': 3,\n",
       "  'Off Boarding': 4,\n",
       "  'CCTV': 5,\n",
       "  'Network': 6,\n",
       "  'Network and Infrastructure Design': 7,\n",
       "  'Circuit': 8,\n",
       "  'Outage': 9,\n",
       "  'Network Security': 10,\n",
       "  'Notification': 11,\n",
       "  'System Security': 12,\n",
       "  'Server Management': 13,\n",
       "  'Server Projects': 14,\n",
       "  'Services Management': 15,\n",
       "  'User Account': 16,\n",
       "  'Checklist': 17,\n",
       "  'Deployment / Movement': 18,\n",
       "  'CASA': 19,\n",
       "  'Email': 20,\n",
       "  'General Assistance': 21,\n",
       "  'Hardware': 22,\n",
       "  'Printer': 23,\n",
       "  'Software': 24,\n",
       "  'Telephony': 25,\n",
       "  'TV Advertisement': 26,\n",
       "  'Development': 27,\n",
       "  'New Hire Shadowing': 28,\n",
       "  'Web/Application Security': 29,\n",
       "  'On Boarding': 30,\n",
       "  'BCP Activation': 31,\n",
       "  'Ticketing Support': 32,\n",
       "  'Email Security': 33,\n",
       "  'TPMO': 34,\n",
       "  'Food Allowance': 35,\n",
       "  'Procurement': 36,\n",
       "  'Compliance': 37,\n",
       "  '__index_level_0__': 38},\n",
       " {0: 'Biometric',\n",
       "  1: 'Document',\n",
       "  2: 'Asset Management',\n",
       "  3: 'Data Security Incident',\n",
       "  4: 'Off Boarding',\n",
       "  5: 'CCTV',\n",
       "  6: 'Network',\n",
       "  7: 'Network and Infrastructure Design',\n",
       "  8: 'Circuit',\n",
       "  9: 'Outage',\n",
       "  10: 'Network Security',\n",
       "  11: 'Notification',\n",
       "  12: 'System Security',\n",
       "  13: 'Server Management',\n",
       "  14: 'Server Projects',\n",
       "  15: 'Services Management',\n",
       "  16: 'User Account',\n",
       "  17: 'Checklist',\n",
       "  18: 'Deployment / Movement',\n",
       "  19: 'CASA',\n",
       "  20: 'Email',\n",
       "  21: 'General Assistance',\n",
       "  22: 'Hardware',\n",
       "  23: 'Printer',\n",
       "  24: 'Software',\n",
       "  25: 'Telephony',\n",
       "  26: 'TV Advertisement',\n",
       "  27: 'Development',\n",
       "  28: 'New Hire Shadowing',\n",
       "  29: 'Web/Application Security',\n",
       "  30: 'On Boarding',\n",
       "  31: 'BCP Activation',\n",
       "  32: 'Ticketing Support',\n",
       "  33: 'Email Security',\n",
       "  34: 'TPMO',\n",
       "  35: 'Food Allowance',\n",
       "  36: 'Procurement',\n",
       "  37: 'Compliance',\n",
       "  38: '__index_level_0__'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'microsoft/deberta-v3-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset, tokenized_test_dataset = preprocess_function(train_dataset,test_dataset,label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 159\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path, num_labels=len(id2label),\n",
    "    id2label=id2label, label2id=label2id,\n",
    "    problem_type=\"multi_label_classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='micro')\n",
    "    precision = precision_score(labels, predictions, average='micro')\n",
    "    recall = recall_score(labels, predictions, average='micro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RestException",
     "evalue": "INVALID_PARAMETER_VALUE: Response: {'Error': {'Code': 'ValidationError', 'Severity': None, 'Message': 'No more than 500 characters per params Value. Request contains 2 of greater length.', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': '246941b1fdbaeab4cba38efa17aae71b', 'request': 'af2ed1b63ff8baca'}, 'Environment': 'southeastasia', 'Location': 'southeastasia', 'Time': '2024-06-21T08:31:41.9784407+00:00', 'ComponentName': 'mlflow', 'statusCode': 400, 'error_code': 'INVALID_PARAMETER_VALUE'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1886\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1887\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1888\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1889\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1890\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/trainer.py:2147\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2144\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m   2145\u001b[0m grad_norm: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallback_handler\u001b[39m.\u001b[39;49mon_train_begin(args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrol)\n\u001b[1;32m   2149\u001b[0m total_batched_samples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   2150\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs_trained, num_train_epochs):\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/trainer_callback.py:454\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_begin\u001b[39m(\u001b[39mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m    453\u001b[0m     control\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_event(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_begin\u001b[39;49m\u001b[39m\"\u001b[39;49m, args, state, control)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/trainer_callback.py:498\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_event\u001b[39m(\u001b[39mself\u001b[39m, event, args, state, control, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    497\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 498\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(callback, event)(\n\u001b[1;32m    499\u001b[0m             args,\n\u001b[1;32m    500\u001b[0m             state,\n\u001b[1;32m    501\u001b[0m             control,\n\u001b[1;32m    502\u001b[0m             model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    503\u001b[0m             tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m    504\u001b[0m             optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer,\n\u001b[1;32m    505\u001b[0m             lr_scheduler\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_scheduler,\n\u001b[1;32m    506\u001b[0m             train_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    507\u001b[0m             eval_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_dataloader,\n\u001b[1;32m    508\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    509\u001b[0m         )\n\u001b[1;32m    510\u001b[0m         \u001b[39m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/integrations/integration_utils.py:1069\u001b[0m, in \u001b[0;36mMLflowCallback.on_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_begin\u001b[39m(\u001b[39mself\u001b[39m, args, state, control, model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1068\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialized:\n\u001b[0;32m-> 1069\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(args, state, model)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/integrations/integration_utils.py:1056\u001b[0m, in \u001b[0;36mMLflowCallback.setup\u001b[0;34m(self, args, state, model)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(combined_dict_items), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_MAX_PARAMS_TAGS_PER_BATCH):\n\u001b[1;32m   1055\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_async_log:\n\u001b[0;32m-> 1056\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ml_flow\u001b[39m.\u001b[39;49mlog_params(\n\u001b[1;32m   1057\u001b[0m             \u001b[39mdict\u001b[39;49m(combined_dict_items[i : i \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_MAX_PARAMS_TAGS_PER_BATCH]), synchronous\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m   1058\u001b[0m         )\n\u001b[1;32m   1059\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1060\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ml_flow\u001b[39m.\u001b[39mlog_params(\u001b[39mdict\u001b[39m(combined_dict_items[i : i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_MAX_PARAMS_TAGS_PER_BATCH]))\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/mlflow/tracking/fluent.py:911\u001b[0m, in \u001b[0;36mlog_params\u001b[0;34m(params, synchronous)\u001b[0m\n\u001b[1;32m    909\u001b[0m params_arr \u001b[39m=\u001b[39m [Param(key, \u001b[39mstr\u001b[39m(value)) \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m    910\u001b[0m synchronous \u001b[39m=\u001b[39m synchronous \u001b[39mif\u001b[39;00m synchronous \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mnot\u001b[39;00m MLFLOW_ENABLE_ASYNC_LOGGING\u001b[39m.\u001b[39mget()\n\u001b[0;32m--> 911\u001b[0m \u001b[39mreturn\u001b[39;00m MlflowClient()\u001b[39m.\u001b[39;49mlog_batch(\n\u001b[1;32m    912\u001b[0m     run_id\u001b[39m=\u001b[39;49mrun_id, metrics\u001b[39m=\u001b[39;49m[], params\u001b[39m=\u001b[39;49mparams_arr, tags\u001b[39m=\u001b[39;49m[], synchronous\u001b[39m=\u001b[39;49msynchronous\n\u001b[1;32m    913\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/mlflow/tracking/client.py:1134\u001b[0m, in \u001b[0;36mMlflowClient.log_batch\u001b[0;34m(self, run_id, metrics, params, tags, synchronous)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_batch\u001b[39m(\n\u001b[1;32m   1062\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1063\u001b[0m     run_id: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     synchronous: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1068\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[RunOperations]:\n\u001b[1;32m   1069\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    Log multiple metrics, params, and/or tags.\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \n\u001b[1;32m   1133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1134\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mlog_batch(\n\u001b[1;32m   1135\u001b[0m         run_id, metrics, params, tags, synchronous\u001b[39m=\u001b[39;49msynchronous\n\u001b[1;32m   1136\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/client.py:476\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_batch\u001b[0;34m(self, run_id, metrics, params, tags, synchronous)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mlog_batch(\n\u001b[1;32m    472\u001b[0m             run_id\u001b[39m=\u001b[39mrun_id, metrics\u001b[39m=\u001b[39mmetrics_batch, params\u001b[39m=\u001b[39mparams_batch, tags\u001b[39m=\u001b[39mtags_batch\n\u001b[1;32m    473\u001b[0m         )\n\u001b[1;32m    474\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m         run_operations_list\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 476\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mlog_batch_async(\n\u001b[1;32m    477\u001b[0m                 run_id\u001b[39m=\u001b[39;49mrun_id,\n\u001b[1;32m    478\u001b[0m                 metrics\u001b[39m=\u001b[39;49mmetrics_batch,\n\u001b[1;32m    479\u001b[0m                 params\u001b[39m=\u001b[39;49mparams_batch,\n\u001b[1;32m    480\u001b[0m                 tags\u001b[39m=\u001b[39;49mtags_batch,\n\u001b[1;32m    481\u001b[0m             )\n\u001b[1;32m    482\u001b[0m         )\n\u001b[1;32m    484\u001b[0m \u001b[39mfor\u001b[39;00m metrics_batch \u001b[39min\u001b[39;00m chunk_list(metrics, chunk_size\u001b[39m=\u001b[39mMAX_METRICS_PER_BATCH):\n\u001b[1;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m synchronous:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/azureml/mlflow/_store/tracking/store.py:92\u001b[0m, in \u001b[0;36mAzureMLRestStore.log_batch_async\u001b[0;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[1;32m     83\u001b[0m tag_protos \u001b[39m=\u001b[39m [tag\u001b[39m.\u001b[39mto_proto() \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tags]\n\u001b[1;32m     84\u001b[0m req_body \u001b[39m=\u001b[39m message_to_json(\n\u001b[1;32m     85\u001b[0m     LogBatchAsync(\n\u001b[1;32m     86\u001b[0m         metrics\u001b[39m=\u001b[39mmetric_protos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m )\n\u001b[0;32m---> 92\u001b[0m response_proto \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_to_endpoint(LogBatchAsync, req_body)\n\u001b[1;32m     93\u001b[0m response \u001b[39m=\u001b[39m LogBatchAsync\u001b[39m.\u001b[39mResponse()\n\u001b[1;32m     94\u001b[0m response\u001b[39m.\u001b[39mbatch_tracking_id \u001b[39m=\u001b[39m response_proto\u001b[39m.\u001b[39mbatch_tracking_id\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/azureml/mlflow/_store/tracking/store.py:113\u001b[0m, in \u001b[0;36mAzureMLRestStore._call_to_endpoint\u001b[0;34m(self, api, json_body)\u001b[0m\n\u001b[1;32m    111\u001b[0m endpoint, method \u001b[39m=\u001b[39m _METHOD_TO_INFO_AML[api]\n\u001b[1;32m    112\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[0;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto\n\u001b[1;32m    115\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/mlflow/utils/rest_utils.py:290\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    288\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n\u001b[1;32m    289\u001b[0m     response \u001b[39m=\u001b[39m http_request(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcall_kwargs)\n\u001b[0;32m--> 290\u001b[0m response \u001b[39m=\u001b[39m verify_rest_response(response, endpoint)\n\u001b[1;32m    291\u001b[0m js_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext)\n\u001b[1;32m    292\u001b[0m parse_dict(js_dict\u001b[39m=\u001b[39mjs_dict, message\u001b[39m=\u001b[39mresponse_proto)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/mlflow/utils/rest_utils.py:173\u001b[0m, in \u001b[0;36mverify_rest_response\u001b[0;34m(response, endpoint)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;00m _can_parse_as_json_object(response\u001b[39m.\u001b[39mtext):\n\u001b[0;32m--> 173\u001b[0m         \u001b[39mraise\u001b[39;00m RestException(json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext))\n\u001b[1;32m    174\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         base_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    176\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to endpoint \u001b[39m\u001b[39m{\u001b[39;00mendpoint\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfailed with error code \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m != 200\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m         )\n",
      "\u001b[0;31mRestException\u001b[0m: INVALID_PARAMETER_VALUE: Response: {'Error': {'Code': 'ValidationError', 'Severity': None, 'Message': 'No more than 500 characters per params Value. Request contains 2 of greater length.', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': '246941b1fdbaeab4cba38efa17aae71b', 'request': 'af2ed1b63ff8baca'}, 'Environment': 'southeastasia', 'Location': 'southeastasia', 'Time': '2024-06-21T08:31:41.9784407+00:00', 'ComponentName': 'mlflow', 'statusCode': 400, 'error_code': 'INVALID_PARAMETER_VALUE'}"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "def train_model(tokenized_train_dataset, tokenized_test_dataset, label2id, id2label, model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # Truncate sequences to a maximum length of 500 tokens\n",
    "    def truncate_sequences(batch):\n",
    "        max_length = 500\n",
    "        batch['input_ids'] = [seq[:max_length] for seq in batch['input_ids']]\n",
    "        batch['attention_mask'] = [mask[:max_length]\n",
    "                                   for mask in batch['attention_mask']]\n",
    "        if 'token_type_ids' in batch:\n",
    "            batch['token_type_ids'] = [type_ids[:max_length]\n",
    "                                       for type_ids in batch['token_type_ids']]\n",
    "        return batch\n",
    "\n",
    "    tokenized_train_dataset = tokenized_train_dataset.map(\n",
    "        truncate_sequences, batched=True)\n",
    "    tokenized_test_dataset = tokenized_test_dataset.map(\n",
    "        truncate_sequences, batched=True)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=len(label2id),\n",
    "        label2id=label2id,\n",
    "        id2label=id2label\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs'\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_test_dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2769b9b9e8044b3bc9dae2d3300fd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bf0ceeba1f4eafbec07467c4c9362e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RestException",
     "evalue": "INVALID_PARAMETER_VALUE: Response: {'Error': {'Code': 'ValidationError', 'Severity': None, 'Message': 'No more than 500 characters per params Value. Request contains 2 of greater length.', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': '8d7b7ae85271da20d2d8083609efb7fa', 'request': 'a049252e302d74cc'}, 'Environment': 'southeastasia', 'Location': 'southeastasia', 'Time': '2024-06-21T08:36:13.6327411+00:00', 'ComponentName': 'mlflow', 'statusCode': 400, 'error_code': 'INVALID_PARAMETER_VALUE'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_test_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/deberta-v3-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 48\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(tokenized_train_dataset, tokenized_test_dataset, label2id, id2label, model_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     31\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     42\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     43\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     44\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,\n\u001b[1;32m     45\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_test_dataset\n\u001b[1;32m     46\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1886\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1887\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1888\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1889\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1890\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/trainer.py:2147\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2144\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m   2145\u001b[0m grad_norm: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallback_handler\u001b[39m.\u001b[39;49mon_train_begin(args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrol)\n\u001b[1;32m   2149\u001b[0m total_batched_samples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   2150\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs_trained, num_train_epochs):\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/trainer_callback.py:454\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_begin\u001b[39m(\u001b[39mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m    453\u001b[0m     control\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_event(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_begin\u001b[39;49m\u001b[39m\"\u001b[39;49m, args, state, control)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/trainer_callback.py:498\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_event\u001b[39m(\u001b[39mself\u001b[39m, event, args, state, control, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    497\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 498\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(callback, event)(\n\u001b[1;32m    499\u001b[0m             args,\n\u001b[1;32m    500\u001b[0m             state,\n\u001b[1;32m    501\u001b[0m             control,\n\u001b[1;32m    502\u001b[0m             model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    503\u001b[0m             tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m    504\u001b[0m             optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer,\n\u001b[1;32m    505\u001b[0m             lr_scheduler\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_scheduler,\n\u001b[1;32m    506\u001b[0m             train_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    507\u001b[0m             eval_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_dataloader,\n\u001b[1;32m    508\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    509\u001b[0m         )\n\u001b[1;32m    510\u001b[0m         \u001b[39m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/integrations/integration_utils.py:1069\u001b[0m, in \u001b[0;36mMLflowCallback.on_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_begin\u001b[39m(\u001b[39mself\u001b[39m, args, state, control, model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1068\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialized:\n\u001b[0;32m-> 1069\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(args, state, model)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/transformers/integrations/integration_utils.py:1056\u001b[0m, in \u001b[0;36mMLflowCallback.setup\u001b[0;34m(self, args, state, model)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(combined_dict_items), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_MAX_PARAMS_TAGS_PER_BATCH):\n\u001b[1;32m   1055\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_async_log:\n\u001b[0;32m-> 1056\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ml_flow\u001b[39m.\u001b[39;49mlog_params(\n\u001b[1;32m   1057\u001b[0m             \u001b[39mdict\u001b[39;49m(combined_dict_items[i : i \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_MAX_PARAMS_TAGS_PER_BATCH]), synchronous\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m   1058\u001b[0m         )\n\u001b[1;32m   1059\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1060\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ml_flow\u001b[39m.\u001b[39mlog_params(\u001b[39mdict\u001b[39m(combined_dict_items[i : i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_MAX_PARAMS_TAGS_PER_BATCH]))\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/mlflow/tracking/fluent.py:911\u001b[0m, in \u001b[0;36mlog_params\u001b[0;34m(params, synchronous)\u001b[0m\n\u001b[1;32m    909\u001b[0m params_arr \u001b[39m=\u001b[39m [Param(key, \u001b[39mstr\u001b[39m(value)) \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m    910\u001b[0m synchronous \u001b[39m=\u001b[39m synchronous \u001b[39mif\u001b[39;00m synchronous \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mnot\u001b[39;00m MLFLOW_ENABLE_ASYNC_LOGGING\u001b[39m.\u001b[39mget()\n\u001b[0;32m--> 911\u001b[0m \u001b[39mreturn\u001b[39;00m MlflowClient()\u001b[39m.\u001b[39;49mlog_batch(\n\u001b[1;32m    912\u001b[0m     run_id\u001b[39m=\u001b[39;49mrun_id, metrics\u001b[39m=\u001b[39;49m[], params\u001b[39m=\u001b[39;49mparams_arr, tags\u001b[39m=\u001b[39;49m[], synchronous\u001b[39m=\u001b[39;49msynchronous\n\u001b[1;32m    913\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/mlflow/tracking/client.py:1134\u001b[0m, in \u001b[0;36mMlflowClient.log_batch\u001b[0;34m(self, run_id, metrics, params, tags, synchronous)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_batch\u001b[39m(\n\u001b[1;32m   1062\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1063\u001b[0m     run_id: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     synchronous: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1068\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[RunOperations]:\n\u001b[1;32m   1069\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    Log multiple metrics, params, and/or tags.\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \n\u001b[1;32m   1133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1134\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mlog_batch(\n\u001b[1;32m   1135\u001b[0m         run_id, metrics, params, tags, synchronous\u001b[39m=\u001b[39;49msynchronous\n\u001b[1;32m   1136\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/client.py:476\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_batch\u001b[0;34m(self, run_id, metrics, params, tags, synchronous)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mlog_batch(\n\u001b[1;32m    472\u001b[0m             run_id\u001b[39m=\u001b[39mrun_id, metrics\u001b[39m=\u001b[39mmetrics_batch, params\u001b[39m=\u001b[39mparams_batch, tags\u001b[39m=\u001b[39mtags_batch\n\u001b[1;32m    473\u001b[0m         )\n\u001b[1;32m    474\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m         run_operations_list\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 476\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mlog_batch_async(\n\u001b[1;32m    477\u001b[0m                 run_id\u001b[39m=\u001b[39;49mrun_id,\n\u001b[1;32m    478\u001b[0m                 metrics\u001b[39m=\u001b[39;49mmetrics_batch,\n\u001b[1;32m    479\u001b[0m                 params\u001b[39m=\u001b[39;49mparams_batch,\n\u001b[1;32m    480\u001b[0m                 tags\u001b[39m=\u001b[39;49mtags_batch,\n\u001b[1;32m    481\u001b[0m             )\n\u001b[1;32m    482\u001b[0m         )\n\u001b[1;32m    484\u001b[0m \u001b[39mfor\u001b[39;00m metrics_batch \u001b[39min\u001b[39;00m chunk_list(metrics, chunk_size\u001b[39m=\u001b[39mMAX_METRICS_PER_BATCH):\n\u001b[1;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m synchronous:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/azureml/mlflow/_store/tracking/store.py:92\u001b[0m, in \u001b[0;36mAzureMLRestStore.log_batch_async\u001b[0;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[1;32m     83\u001b[0m tag_protos \u001b[39m=\u001b[39m [tag\u001b[39m.\u001b[39mto_proto() \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tags]\n\u001b[1;32m     84\u001b[0m req_body \u001b[39m=\u001b[39m message_to_json(\n\u001b[1;32m     85\u001b[0m     LogBatchAsync(\n\u001b[1;32m     86\u001b[0m         metrics\u001b[39m=\u001b[39mmetric_protos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m )\n\u001b[0;32m---> 92\u001b[0m response_proto \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_to_endpoint(LogBatchAsync, req_body)\n\u001b[1;32m     93\u001b[0m response \u001b[39m=\u001b[39m LogBatchAsync\u001b[39m.\u001b[39mResponse()\n\u001b[1;32m     94\u001b[0m response\u001b[39m.\u001b[39mbatch_tracking_id \u001b[39m=\u001b[39m response_proto\u001b[39m.\u001b[39mbatch_tracking_id\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/azureml/mlflow/_store/tracking/store.py:113\u001b[0m, in \u001b[0;36mAzureMLRestStore._call_to_endpoint\u001b[0;34m(self, api, json_body)\u001b[0m\n\u001b[1;32m    111\u001b[0m endpoint, method \u001b[39m=\u001b[39m _METHOD_TO_INFO_AML[api]\n\u001b[1;32m    112\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[0;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto\n\u001b[1;32m    115\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/mlflow/utils/rest_utils.py:290\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    288\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json_body\n\u001b[1;32m    289\u001b[0m     response \u001b[39m=\u001b[39m http_request(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcall_kwargs)\n\u001b[0;32m--> 290\u001b[0m response \u001b[39m=\u001b[39m verify_rest_response(response, endpoint)\n\u001b[1;32m    291\u001b[0m js_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext)\n\u001b[1;32m    292\u001b[0m parse_dict(js_dict\u001b[39m=\u001b[39mjs_dict, message\u001b[39m=\u001b[39mresponse_proto)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.9/site-packages/mlflow/utils/rest_utils.py:173\u001b[0m, in \u001b[0;36mverify_rest_response\u001b[0;34m(response, endpoint)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;00m _can_parse_as_json_object(response\u001b[39m.\u001b[39mtext):\n\u001b[0;32m--> 173\u001b[0m         \u001b[39mraise\u001b[39;00m RestException(json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext))\n\u001b[1;32m    174\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         base_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    176\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to endpoint \u001b[39m\u001b[39m{\u001b[39;00mendpoint\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfailed with error code \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m != 200\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m         )\n",
      "\u001b[0;31mRestException\u001b[0m: INVALID_PARAMETER_VALUE: Response: {'Error': {'Code': 'ValidationError', 'Severity': None, 'Message': 'No more than 500 characters per params Value. Request contains 2 of greater length.', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': '8d7b7ae85271da20d2d8083609efb7fa', 'request': 'a049252e302d74cc'}, 'Environment': 'southeastasia', 'Location': 'southeastasia', 'Time': '2024-06-21T08:36:13.6327411+00:00', 'ComponentName': 'mlflow', 'statusCode': 400, 'error_code': 'INVALID_PARAMETER_VALUE'}"
     ]
    }
   ],
   "source": [
    "train_model(tokenized_train_dataset, tokenized_test_dataset,\n",
    "            label2id, id2label, model_path=\"microsoft/deberta-v3-small\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('azureml_py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
